{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Analysis Pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VCKhqBaK3PzzW9bLI4EluctYjXSFbQ-D",
      "authorship_tag": "ABX9TyOo8H1QK0jMEQF2I1Ww1T27"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EETk0sY-vNsL"
      },
      "source": [
        "# Twitter sentiment analysis and prediction using pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_QbjB5HpxQX"
      },
      "source": [
        "from IPython import display\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WmlWWKRn2ce"
      },
      "source": [
        "\n",
        "sc =SparkContext()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsmorFLCn2fU"
      },
      "source": [
        "customSchema = StructType([\n",
        "    StructField(\"clean_text\", StringType()), \n",
        "    StructField(\"category\", StringType())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDKoOlZ8n2jl"
      },
      "source": [
        "#modi_data.csv file contains 10000 tweets with seach query modi\n",
        "filename = '/content/drive/MyDrive/twtr_dataset.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwAfkmjVn2mj"
      },
      "source": [
        "df = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woLwFnHtohRX",
        "outputId": "d012505c-3097-47c8-dcba-28956571cf6d"
      },
      "source": [
        "\n",
        "data = df.na.drop(how='any')\n",
        "data.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+\n",
            "|          clean_text|category|\n",
            "+--------------------+--------+\n",
            "|when modi promise...|      -1|\n",
            "|talk all the nons...|       0|\n",
            "|what did just say...|       1|\n",
            "|asking his suppor...|       1|\n",
            "|answer who among ...|       1|\n",
            "+--------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tLwI1caohVE",
        "outputId": "67890a17-b46e-4d10-eda5-8a1b1161541c"
      },
      "source": [
        "data.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- clean_text: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6kFZ0rlvV0C"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElDd8hy9ohYH",
        "outputId": "13181590-da6e-431c-b528-c27710d78965"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "data.groupBy(\"category\").count().orderBy(col(\"count\").desc()).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|       1|70475|\n",
            "|       0|53551|\n",
            "|      -1|34664|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxXBz7WuvfDp"
      },
      "source": [
        "\n",
        "#Model Pipeline\n",
        "Spark Machine Learning Pipelines API is similar to Scikit-Learn. Our pipeline includes three steps:\n",
        "\n",
        "regexTokenizer: Tokenization (with Regular Expression)\n",
        "\n",
        "stopwordsRemover: Remove Stop Words\n",
        "\n",
        "countVectors: Count vectors (“document-term vectors”)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJuByx0zoha-"
      },
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# regular expression tokenizer\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"clean_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "# stop words\n",
        "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
        "\n",
        "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
        "\n",
        "# bag of words count\n",
        "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=20000, minDF=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je5SvSdXqEEG",
        "outputId": "f3817742-535a-483e-f460-907c208f4c23"
      },
      "source": [
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
        "\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "dataset.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "|          clean_text|category|               words|            filtered|            features|label|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "|when modi promise...|      -1|[when, modi, prom...|[when, modi, prom...|(19479,[0,1,4,30,...|  2.0|\n",
            "|talk all the nons...|       0|[talk, all, the, ...|[talk, all, nonse...|(19479,[0,1,2,5,1...|  1.0|\n",
            "|what did just say...|       1|[what, did, just,...|[what, did, just,...|(19479,[0,2,3,19,...|  0.0|\n",
            "|asking his suppor...|       1|[asking, his, sup...|[asking, his, sup...|(19479,[0,4,5,9,1...|  0.0|\n",
            "|answer who among ...|       1|[answer, who, amo...|[answer, who, amo...|(19479,[0,20,78,1...|  0.0|\n",
            "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQZr8OyavoyH"
      },
      "source": [
        "# Partition Training & Test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBZLyg59qE2F",
        "outputId": "d226dad7-4bee-4e7e-af1c-d6ea51f315ea"
      },
      "source": [
        "\n",
        "# set seed for reproducibility\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 111062\n",
            "Test Dataset Count: 47628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvcPLnjzvvLI"
      },
      "source": [
        "#Model Training and Evaluation\n",
        "Logistic Regression using Count Vector Features\n",
        "\n",
        "Our model will make predictions and score on the test set; we then look at the top 10 predictions from the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jnnrT25qE5H",
        "outputId": "a3edc185-7ee5-4c31-d825-dae0daa24a25"
      },
      "source": [
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\")\\\n",
        ".orderBy(\"probability\", ascending=False).show(n = 10, truncate = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|agree take crisis 2008 tank...|       1|[0.9991513253573526,1.65713...|  0.0|       0.0|\n",
            "|much love too the beautiful...|       1|[0.9990368857446261,2.91859...|  0.0|       0.0|\n",
            "|chacha modi new vision have...|       1|[0.9990354345977572,1.00719...|  0.0|       0.0|\n",
            "|appeasement hai mumkeen hai...|       1|[0.9988034011551032,3.62290...|  0.0|       0.0|\n",
            "|very true our country needs...|       1|[0.9984626558069003,1.77380...|  0.0|       0.0|\n",
            "|modi will wish you many man...|       1|[0.9983693606015881,7.48362...|  0.0|       0.0|\n",
            "|india becoming superhero na...|       1|[0.9979699958557681,5.29687...|  0.0|       0.0|\n",
            "|have chapter this great per...|       1|[0.9979261076257133,3.33235...|  0.0|       0.0|\n",
            "|the very dynamic smart wise...|       1|[0.9977068435887573,9.19328...|  0.0|       0.0|\n",
            "|can modi win can modi win k...|       1|[0.9976597609004368,0.00144...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLSAxwNCqE_m",
        "outputId": "c9a3c0d8-45fa-46ca-de2b-2648cbd4fdc7"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7885893153659636"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "darP6M8mv50v"
      },
      "source": [
        "\n",
        "# Logistic Regression using TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnbkLSHlqKGc",
        "outputId": "10d71606-61a9-4197-dc82-1f729b26b0c2"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "lrModel = lr.fit(trainingData)\n",
        "\n",
        "predictions = lrModel.transform(testData)\n",
        "\n",
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|                    clean_text|category|                   probability|label|prediction|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "|agree take crisis 2008 tank...|       1|[0.9991752951148799,4.16452...|  0.0|       0.0|\n",
            "|appeasement hai mumkeen hai...|       1|[0.9988554034429974,3.64709...|  0.0|       0.0|\n",
            "|chacha modi new vision have...|       1|[0.9980556769534737,2.50636...|  0.0|       0.0|\n",
            "|vote for pappu straight mig...|       1|[0.9980066892297601,2.91804...|  0.0|       0.0|\n",
            "|can modi win can modi win k...|       1|[0.9977329090810103,0.00119...|  0.0|       0.0|\n",
            "|much love too the beautiful...|       1|[0.9975824836038859,6.52193...|  0.0|       0.0|\n",
            "|arnab sir nation wants know...|       1|[0.9974563163425116,1.90441...|  0.0|       0.0|\n",
            "|very true our country needs...|       1|[0.9972855347002976,2.36914...|  0.0|       0.0|\n",
            "|much more than expected mor...|       1|[0.9970409637436906,1.61097...|  0.0|       0.0|\n",
            "|have chapter this great per...|       1|[0.9969775163958017,5.02217...|  0.0|       0.0|\n",
            "+------------------------------+--------+------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od1-tOEYqKJY",
        "outputId": "1aa1fb9b-2a9a-45d7-93bc-4b93b1ac8aa2"
      },
      "source": [
        "\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.752506587829302"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAykwM3Uv_h6"
      },
      "source": [
        "\n",
        "#Cross-Validation\n",
        "Let’s now try cross-validation to tune our hyper parameters, and we will only tune the count vectors Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV9Y7tFoqSi1"
      },
      "source": [
        "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
        "\n",
        "pipelineFit = pipeline.fit(data)\n",
        "dataset = pipelineFit.transform(data)\n",
        "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
        "\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3h4025NqSlx",
        "outputId": "2ce95fc3-a752-428a-f8ef-b528a5ed5f96"
      },
      "source": [
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Create ParamGrid for Cross Validation\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
        "             .build())\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "cv = CrossValidator(estimator=lr, \\\n",
        "                    estimatorParamMaps=paramGrid, \\\n",
        "                    evaluator=evaluator, \\\n",
        "                    numFolds=5)\n",
        "\n",
        "cvModel = cv.fit(trainingData)\n",
        "\n",
        "predictions = cvModel.transform(testData)\n",
        "# Evaluate best model\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.836646950243763"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBoH1jmTqSrP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}